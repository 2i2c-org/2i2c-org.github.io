<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Incident-Report | 2i2c</title><link>https://2i2c.org/category/incident-report/</link><atom:link href="https://2i2c.org/category/incident-report/index.xml" rel="self" type="application/rss+xml"/><description>Incident-Report</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 16 Sep 2025 00:00:00 +0000</lastBuildDate><image><url>https://2i2c.org/media/sharing.png</url><title>Incident-Report</title><link>https://2i2c.org/category/incident-report/</link></image><item><title>Incident report: UC Merced user throttling during class startup</title><link>https://2i2c.org/blog/2025/incident-ucmerced-user-throttling/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://2i2c.org/blog/2025/incident-ucmerced-user-throttling/</guid><description>&lt;p>On August 29, 2025 our cloud infrastructure team experienced an incident with the UC Merced community hub when students tried to login simultaneously at the start of class. For more detailed technical information about this incident, see our
&lt;a href="https://github.com/2i2c-org/incident-reports/blob/main/reports/2025-08-29-ucmerced-too-many-users-throttled.pdf" target="_blank" rel="noopener" >full incident report&lt;/a>.&lt;/p>
&lt;h2 id="what-happened">
What happened
&lt;a class="header-anchor" href="#what-happened">#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>Students experienced issues when trying to login to the hub at the same time during the start of class.&lt;/li>
&lt;li>The concurrent spawn limit was reached quickly due to the large number of users starting up simultaneously.&lt;/li>
&lt;li>New nodes had to be brought up by the autoscaler, which took roughly 10 minutes from start to end.&lt;/li>
&lt;li>Users who tried again after 1 minute weren&amp;rsquo;t guaranteed to get their servers started immediately since new nodes were still spinning up.&lt;/li>
&lt;li>This was an &amp;ldquo;expected&amp;rdquo; scale-up event but the lack of clear messaging caused users to interpret it as instability.&lt;/li>
&lt;/ul>
&lt;h2 id="what-we-learned">
What we learned
&lt;a class="header-anchor" href="#what-we-learned">#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>We need better communication so users understand when infrastructure slowness is &amp;ldquo;expected&amp;rdquo; vs. &amp;ldquo;unstable&amp;rdquo;.&lt;/li>
&lt;li>We need better alerting for concurrent user startup throttling - we found out about this issue from users rather than automated monitoring.&lt;/li>
&lt;li>We learned that JupyterHub&amp;rsquo;s metrics don&amp;rsquo;t properly expose &lt;code>429 status&lt;/code> codes in our dashboards.&lt;/li>
&lt;li>This will happen again if we don&amp;rsquo;t have proper scaling limits and node provisioning strategies for sudden user influxes.&lt;/li>
&lt;/ul>
&lt;h2 id="resolution">
Resolution
&lt;a class="header-anchor" href="#resolution">#&lt;/a>
&lt;/h2>&lt;p>We implemented several fixes:&lt;/p>
&lt;ul>
&lt;li>Increased the concurrent spawn limit from 64 to 100.&lt;/li>
&lt;li>Put UC Merced users on larger nodes to reduce the number of node spinups needed. this will cost more in cloud but result in fewer scale-up events.&lt;/li>
&lt;li>Created action items to improve logging, alerting, and monitoring for similar incidents&lt;/li>
&lt;/ul>
&lt;h2 id="acknowledgements">
Acknowledgements
&lt;a class="header-anchor" href="#acknowledgements">#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>Thanks to UC Merced students and instructors for reporting the issue through our support system.&lt;/li>
&lt;/ul></description></item></channel></rss>