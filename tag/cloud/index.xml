<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cloud | 2i2c</title><link>https://2i2c.org/tag/cloud/</link><atom:link href="https://2i2c.org/tag/cloud/index.xml" rel="self" type="application/rss+xml"/><description>Cloud</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 14 Oct 2025 00:00:00 +0000</lastBuildDate><image><url>https://2i2c.org/media/sharing.png</url><title>Cloud</title><link>https://2i2c.org/tag/cloud/</link></image><item><title>Impact report from 2i2c's Binder federation instance</title><link>https://2i2c.org/blog/binder-report/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://2i2c.org/blog/binder-report/</guid><description>&lt;p>The
&lt;a href="https://mybinder.org" target="_blank" rel="noopener" >mybinder.org&lt;/a> service provides reproducible and interactive computational environments for the open science community. It is financially supported by a
&lt;a href="https://mybinder.readthedocs.io/en/latest/about/federation.html" target="_blank" rel="noopener" >federation of BinderHubs&lt;/a>.
2i2c is part of the team that manages mybinder.org, and we&amp;rsquo;re committed to supporting this critical infrastructure for open science and reproducibility.&lt;/p>
&lt;p>We developed a more
&lt;a href="https://2i2c.org/blog/binder-report/../binder-singlenode/" >cost-efficient process for deploying BinderHub on a single VM&lt;/a> and are now running a BinderHub at
&lt;a href="https://2i2c.mybinder.org" target="_blank" rel="noopener" >2i2c.mybinder.org&lt;/a>.
This post highlights the impact we&amp;rsquo;ve had through our support of the mybinder.org federation, we&amp;rsquo;ll update it periodically.&lt;/p>
&lt;h2 id="usage-over-time">
Usage over time
&lt;a class="header-anchor" href="#usage-over-time">#&lt;/a>
&lt;/h2>&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Binder launches" srcset="
/blog/binder-report/featured_hu0f9f1ae8d80fa109b602b80004fe12f0_156400_bd3dc558251d8d2f9ceb45632df1537f.webp 400w,
/blog/binder-report/featured_hu0f9f1ae8d80fa109b602b80004fe12f0_156400_43138623992843945bb647029e00a3e0.webp 760w,
/blog/binder-report/featured_hu0f9f1ae8d80fa109b602b80004fe12f0_156400_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://2i2c.org/blog/binder-report/featured_hu0f9f1ae8d80fa109b602b80004fe12f0_156400_bd3dc558251d8d2f9ceb45632df1537f.webp"
width="760"
height="673"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Here are weekly launches on mybinder.org. Launches from &lt;code>2i2c.mybinder.org&lt;/code> are in red. Source:
&lt;a href="https://hub.jupyter.org/binder-data/" target="_blank" rel="noopener" >mybinder analytics dashboard&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;strong>In Q1 of 2025&lt;/strong>, &lt;code>2i2c.mybinder.org&lt;/code> launched &lt;strong>417,048 reproducible sessions&lt;/strong>. In this time, mybinder.org was primarily driven by our
&lt;a href="https://2i2c.org/blog/binder-report/../binder-singlenode/" >new Hetzner node&lt;/a> as we worked with GESIS to stabilize their own BinderHub instance.&lt;/p>
&lt;p>&lt;strong>In Q2 of 2025&lt;/strong>, &lt;code>2i2c.mybinder.org&lt;/code> launched &lt;strong>249,750 reproducible sessions&lt;/strong>. In this time, we worked with
&lt;a href="https://2i2c.org/collaborators/gesis/" >GESIS&lt;/a> to deploy their BinderHub instance on the same Hetzner node setup, which let us distribute more of Binder&amp;rsquo;s load onto them.&lt;/p>
&lt;p>&lt;strong>In Q3 of 2025&lt;/strong>, &lt;code>2i2c.mybinder.org&lt;/code> launched &lt;strong>118,083 reproducible sessions&lt;/strong>. We experienced a typical summer dip in sessions, had to
&lt;a href="https://2i2c.org/blog/binder-report/../mybinder-analytics-fix/" >fix the analytics dashboard&lt;/a> as well as
&lt;a href="https://2i2c.org/blog/binder-report/../mybinder-antiabuse-scanning/" >fix a TCP scanning abuse case&lt;/a> that briefly brought down the 2i2c node.&lt;/p>
&lt;h2 id="where-weve-made-improvements">
Where we&amp;rsquo;ve made improvements
&lt;a class="header-anchor" href="#where-weve-made-improvements">#&lt;/a>
&lt;/h2>&lt;p>As part of this effort, we&amp;rsquo;ve made several improvements to the Binder and JupyterHub ecosystem. Here are a few links where you can read more:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://2i2c.org/blog/binder-report/../binder-singlenode/" >Deploying BinderHub on a single VM with k3s&lt;/a> - Our approach to making BinderHub deployment cheaper and simpler&lt;/li>
&lt;li>
&lt;a href="https://2i2c.org/blog/binder-report/../jetstream-binderhub/" >Hetzner cloud infrastructure experience&lt;/a> - Cost-effective cloud hosting for mybinder.org&lt;/li>
&lt;li>
&lt;a href="https://2i2c.org/blog/binder-report/../mybinder-antiabuse-scanning/" >Combating TCP scanning abuse on mybinder.org&lt;/a> - Developing anti-abuse tools to prevent an abuse use-case.&lt;/li>
&lt;li>
&lt;a href="https://hub.jupyter.org/binder-data/" target="_blank" rel="noopener" >Improving Binder&amp;rsquo;s usage dashboard&lt;/a> - This helps us create posts like these and is useful to others as well.&lt;/li>
&lt;li>
&lt;a href="https://2i2c.org/blog/binder-report/../../2024/jupyterhub-binderhub-gesis/" >Integrating BinderHub with JupyterHub&lt;/a> - Working with
&lt;a href="https://2i2c.org/collaborators/gesis/" >GESIS&lt;/a> to bring Binder&amp;rsquo;s dynamic image building capabilities to persistent JupyterHubs, empowering users to manage their own environments&lt;/li>
&lt;/ul>
&lt;h2 id="acknowledgements">
Acknowledgements
&lt;a class="header-anchor" href="#acknowledgements">#&lt;/a>
&lt;/h2>&lt;p>This work is made possible by:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://2i2c.org/collaborators/gesis/" >GESIS&lt;/a> for their continued support as mybinder.org federation members&lt;/li>
&lt;li>The
&lt;a href="https://2i2c.org/collaborators/jupyterhub/" >JupyterHub community&lt;/a> for collaboration and support&lt;/li>
&lt;li>Our
&lt;a href="https://2i2c.org/members/" >member communities&lt;/a> whose fees support this work&lt;/li>
&lt;/ul></description></item><item><title>Incident report: UC Merced user throttling during class startup</title><link>https://2i2c.org/blog/incident-ucmerced-user-throttling/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://2i2c.org/blog/incident-ucmerced-user-throttling/</guid><description>&lt;p>On August 29, 2025 our cloud infrastructure team experienced an incident with the UC Merced community hub when students tried to login simultaneously at the start of class. For more detailed technical information about this incident, see our
&lt;a href="https://github.com/2i2c-org/incident-reports/blob/main/reports/2025-08-29-ucmerced-too-many-users-throttled.pdf" target="_blank" rel="noopener" >full incident report&lt;/a>.&lt;/p>
&lt;h2 id="what-happened">
What happened
&lt;a class="header-anchor" href="#what-happened">#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>Students experienced issues when trying to login to the hub at the same time during the start of class.&lt;/li>
&lt;li>The concurrent spawn limit was reached quickly due to the large number of users starting up simultaneously.&lt;/li>
&lt;li>New nodes had to be brought up by the autoscaler, which took roughly 10 minutes from start to end.&lt;/li>
&lt;li>Users who tried again after 1 minute weren&amp;rsquo;t guaranteed to get their servers started immediately since new nodes were still spinning up.&lt;/li>
&lt;li>This was an &amp;ldquo;expected&amp;rdquo; scale-up event but the lack of clear messaging caused users to interpret it as instability.&lt;/li>
&lt;/ul>
&lt;h2 id="what-we-learned">
What we learned
&lt;a class="header-anchor" href="#what-we-learned">#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>We need better communication so users understand when infrastructure slowness is &amp;ldquo;expected&amp;rdquo; vs. &amp;ldquo;unstable&amp;rdquo;.&lt;/li>
&lt;li>We need better alerting for concurrent user startup throttling - we found out about this issue from users rather than automated monitoring.&lt;/li>
&lt;li>We learned that JupyterHub&amp;rsquo;s metrics don&amp;rsquo;t properly expose &lt;code>429 status&lt;/code> codes in our dashboards.&lt;/li>
&lt;li>This will happen again if we don&amp;rsquo;t have proper scaling limits and node provisioning strategies for sudden user influxes.&lt;/li>
&lt;/ul>
&lt;h2 id="resolution">
Resolution
&lt;a class="header-anchor" href="#resolution">#&lt;/a>
&lt;/h2>&lt;p>We implemented several fixes:&lt;/p>
&lt;ul>
&lt;li>Increased the concurrent spawn limit from 64 to 100.&lt;/li>
&lt;li>Put UC Merced users on larger nodes to reduce the number of node spinups needed. this will cost more in cloud but result in fewer scale-up events.&lt;/li>
&lt;li>Created action items to improve logging, alerting, and monitoring for similar incidents&lt;/li>
&lt;/ul>
&lt;h2 id="acknowledgements">
Acknowledgements
&lt;a class="header-anchor" href="#acknowledgements">#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>Thanks to UC Merced students and instructors for reporting the issue through our support system.&lt;/li>
&lt;/ul></description></item></channel></rss>